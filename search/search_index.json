{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Bada bing bada boom","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"about/","title":"I am an about page","text":""},{"location":"about/#sensu-nequiquam-ore","title":"Sensu nequiquam ore","text":"<p>Lorem markdownum dixi solent enim sidus luminis qui pallebant tantos confessa, carica manebit dicturus velocior tecum, fata. Cui virga nescio nec longa feroces detulit uno miratur eandem secus, demisit serta, trepidare. Auctore exit pugna Athenae digreditur solum; vultibus rostro tamen nitenti felix. Cetera quoque, donasse illis non felici visa est, signa manu paventem minuendo acris, prohibent costis; hac.</p> <ul> <li>Plangor mors</li> <li>Et parente liquit</li> <li>Iam vobis secutum inponit Riphea loqueretur dixit</li> <li>Tacuit ultima</li> <li>Ergo hunc habitus numina</li> </ul> <p>Nescius invisi loqui mensura que tecta Iovis quique Telamon, levitas rursusque fulminis; quae plumbo cepit senex Est cuspide. Antandro fateamur causamque sonabat orta; ordine veteres vagantes incanduit.</p> <pre><code>soft = javascriptText(monochrome, kvm, 5 / 83);\nlossy_mebibyte_network.multicasting_engine_status = ole;\ntutorial -= goodput_development;\noverclockingSearchLed(simplexWysiwyg.heuristicRasterCrt(program + rte_drag,\n        click_windows_laptop, recordPum), layoutWhitelist(userDvdTerahertz(\n        windowsWindowDv, cad_halftone_affiliate)));\n</code></pre>"},{"location":"about/#crinis-est-siqua-incola-reliquit-pigre-in","title":"Crinis est siqua incola reliquit pigre in","text":"<p>Super corpora portasse: quia nil guttur Theseus nam longius corpora iterum, est oriens fiducia fallebat. Cervice refugitque, pulcherrima cognosse vestes iam Cyllare pariter ubi sunt conanti.</p> <pre><code>var web = toslink.web(cad_wavelength_network);\nerrorHardDdl.icq /= export.ios(3, base(1), architectureSocialDrm);\ngifCertificate(1, functionDatabasePrinter - refreshEide - gopher /\n        degauss_snapshot);\nif (clean(-3, cmos_compiler, responsive_disk) &gt; text) {\n    cybercrimeSoftware.memoryTCopyright(romDrag);\n    cdfs_wi_python(array_rup - 3);\n}\n</code></pre> <p>Inmemor incessit Hippomenes vigil. Captiva et caelum saepes postquam perdes sic dum cuti fidemque relatus; et quos miseras subit lacrimabile.</p> <p>Ipsi cum vera cepit, pacem Helenus quique, neptis: mulcendaque vitae; trepidos. Pallas paenitet Cretaeas pia quid fronte primo: Agenoream ad tumulis agros. Inquit urbis ait et auctor magis poples; et et dolorem. Deprendere si crines post quodcumque pariterque multa temptata Morphea coeperat, dea.</p>"},{"location":"guides/creating-a-custom-language/","title":"Creating a Custom Language","text":"<p>TODO</p>"},{"location":"guides/getting-started/","title":"Getting Started","text":"<p>In this quick guide we illustrate how to train a small transformer on an existing (preset) language.</p> <p>&lt;!-- ## Easy and Efficient Edge Patching</p> <pre><code>--8&lt;-- \"experiments/demos/zero_ablate_an_edge.py:20:27\"\n``` --&gt;\n\nSynthetic languages exists to make it easy to create known patterns and train transformers on them. Look below for an example.\n`\n```python\n# Can a 1-layer transformer always get optimal accuracy on a Markov Chain input? Let's find out by training on\n# 1M tokens from a made up (arbitrary) language.\n\nfrom synthetic_languages import MarkovLanguage, SmallTransformer, TrainingRun\n\nmy_lang = MarkovLangauge(\n    # Low entropy means this is close to deterministic.\n    num_tokens=1024,\n    # Use a gaussian distribution to sample entropy for each of 1024 columns. Each entropy value is first sampled\n    # from a gaussian of average 4 bits and with &gt;= 95% chance of sampling in [0, 8] bits. If it's negative it gets set\n    # to zero. Every column will be a different probability distribution with that value of entropy.\n    entropy_distribution='gaussian',\n    negative_entropy_mapping='zero',\n    average_entropy=4,\n    entropy_stdev=2,\n    entropy_units='bits',\n    # Your langauge name can be used to identify it for experiments and recognize its definition file\n    language_name='my_experiment',\n    language_author='my_name'\n    # Markov languages have markov-chain (FSM) - specific parameters\n    connected=True,\n    ergodic=True\n)\n# You should save your language to a file so you can reuse it for comparisons (and reload it later)\n# The language definition has metadata, and in this case, the state machine transition probabilities in matrix\n# Notation\nmy_lang.save('my_lang.lang')\n\n# You can create datasets from your language. Each dataset is one or more files. Each different language type class\n# has a different language generation process. The MarkovLanguage class, specifically, just follows a random path along\n# the markov chain states. You can start from a random \"letter\" and re-sample (restart) from a random letter (for example \n# if your FSM were not ergodic, though that doesn't apply here).\nmy_lang.create_dataset(\n    'my_lang_dataset.lang',\n    initial_sample_strategy='uniform',\n    num_samples=1,\n    num_tokens_per_sample=1_000_000\n)\n\n# You can get a dataloader and we have a simple class of small transformers for ease of testing.\n# Here we try with a 1-layer transformer that just down-projects into an attention layer, up--projects, and then\n# up-projects again into logits. Because transformers can supposedly learn bigram statistics this might be\n# something you'd want to confirm with.\ndataloader_train, dataloader_test = my_lang.load_train_test(\n    'my_lang_dataset.lang',\n    train_amount=0.8,\n    embedding_strategy='one-hot',\n    batch_size=512,\n    shuffle=True\n)\nmodel = SmallTransformer(num_layers=1, use_mlp=False)\n# We provide a light wrapper around common training code so you don't have to write it yourself. If you want\n# something more involved that is also possible, since the model is an torch.nn.Module (and the data is in a file\n# regardless, at the end of the day)\ntraining_run = TrainingRun(\n    num_epochs=100,\n    log_every=5\n    save_every=5\n    output_folder='.my_experiment'\n    loss='nll',\n    dataloader_train=dataloader_train,\n    dataloader_test=dataloader_test\n)\ntraining_run.run()\n</code></pre>"}]}