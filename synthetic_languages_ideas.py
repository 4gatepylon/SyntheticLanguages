import abc
import pydantic
import enum
from typing import Union, List
### THOUGHTS ON SYNTHETIC LANGUAGES ###
# Synthetic Languages has a broad scope but for the initial realease it basically just hopes to support traversals on graphs (i.e. MCs/FSMs) and
# group operations (which, yes, these are graphs but are mathematically meaningful and interesting to people). We should make sure that these cover
# skip bigrams and stuff like that: https://www.youtube.com/watch?v=yo4QvDn-vsU&list=LL&index=36&t=4s&ab_channel=NeelNanda, modular arithmetic,
# the stuff from this repository including its visualization.
# 
# We promise to make description, training, and storage + creation of initial plots and tables very easy (including linear probes and the like i.e. for
# this sort of belief state stuff). Interpretability beyond that is scoped OUT.
# Search algorithms are scoped OUT.
#
#
# Things that should be available in our library:
# 1. More variety of language declarations
# 2. Ability to use stuff like "1M" etc... to translate to numbers when you declare sequence length
# 3. Ability to save to s3, GCP, etc... and be organized
#    Support for WanDB, HF, etc...
# 4. A simple and clear way to declare graphs
#    Ideally support for ways to analyze the graphs... not sure
# 5. Visualization like epsilon transformers
# 6. Visualization from SAE Vis
# 7. Support for SAE training and usage via SAELens
# 8. Support for a well known existing set of graphs
#    - All existing known classes of groups (i.e. all of https://www.youtube.com/watch?v=mH0oCDa74tE&ab_channel=3Blue1Brown, potentially excepting the singletons
#      such as the monster family and the pariahs) + ways to compose them
#    - All markov chains you might want to have + ways to compose them (if we can use Krohn Rhodes and actually understand it that would
#      be great)
#    - A basic set of atoms that I think will be useful and/or interesting TBD like intro to probability random variables that are used to cause changes in
#      state (i.e. probability of going to next state in a sequence)
#    - We should be able to define grid-worlds with some special additional features to use distance random variables; we should support orderings and other
#      such stuff.
#     - NOTE we are going to need to provide a solution for embedding. I think the easiest thing is to do positional embedding and provide everything
#       transformer-lens does, but I understand this will not be ideal in the long run esp. if we are looking at geometric spaces.
# 9. Benchmarking so we can know how fast everything is and tests so we can make sure everything is correct (with proper mockups)
# 10. A clear series of tutorials on how to use the library (in the future I imagined a lot of ways this could expand by creating
#     a hub and other stuff, but that wont happen for now; more importantly the first thing to set up after launch should be sphinx...
#     or maybe during/before launch; in the future I'll need to make a website; future directions may also include audio or video generated by
#     known patterns).
# 11. A bunch of good defaults for everything that could concievably be an experiment that someone might want to run.


### THOUGHTS ON KROHN RHODES ###
# I have not really understood the point of the Krohn Rhodes theorem from here: https://www.youtube.com/watch?v=3b1YRqDQ25w, but I do have some ideas and questions.
# It appears that their construction of a decomposition of FSMs is in terms of using negations of which state you are not in. It's not the most useful for our thinking
# about machine learning for a couple reasons: (1) it requires one decomposition step per state, but we are looking for something akin to a LOGARITHMIC number
# of decomposition steps; (2) each additional decomposition step basically has to integrate the information from the previous steps, which like the previous point, means
# that it doesn't really feel like it's lowering the complexity of the problem into different modules that then compose; (3) it's on deterministic FSMs, but we are
# interested in discovering whether or not we are modeling hidden state PROBABILISTIC state machines. WE WILL BENEFIT GREATLY FROM A DECOMPOSITION, however
# this might not be the right one or am I not understanding its implications properly. The one thing that makes this a good decomposition, IMO, is that it allows you to
# create the FSM from a SMALL and FINITE ALPHABET of elemnets. Ideally our decomposition will contain the following properties:
# - Allowing for minimal error, we can say that the MC is composed of compositions of MCs where, if you are within a child MC, you are minimally likely
#   to leak outwards towards the parent MC. This should be based on the integrated probability of being within each element times the probability of leaking.
# - At each split point there are not too many directions to split into
# - The depth of the decomposition is low.
# - Everything outputs two states: the current-level state and the child state.
# - Ideally, it can somehow capture our notion of compilation introducing shuffling as well as ideas like geometry and circuits/busses that let you move
#   between parts in other ways (i.e. a simple hierarchical split is not always going to be the simplest/best way to see this). It would also be nice to
#   have it learn about accumulation and different stuff like that.

class CanonicalAlphabetDecodingStrategy(enum.Enum):
    LIST_ELEMENT = "LIST_ELEMENT"

class LanguageSpecification(pydantic.BaseModel):
    """
    A language specification specifies the definition of a language. It is meant to be something you can store and load from disk.
    This is meant to be shareable and portable with other users of synthetic_langauges. Different languages (and even sometimes) the
    same one can be stored in different ways so we provide flags via enumerations that allow you to define that.

    The main question we are trying to answer here is "how does an autoregressive model predict the next token?" The approach here is
    to pick langauges for which we know how to predict the next token and use that to help us understand how the transformer learned algorithms
    do it.

    Right now this is for regular languages.
    """
    name: str
    alphabet_length: int
    canonical_alphabet: Union[List[str], str]
    canonical_alphabet_decoding_strategy: CanonicalAlphabetDecodingStrategy = CanonicalAlphabetDecodingStrategy.LIST_ELEMENT

class Language(abc.ABC):
    """
    A language defines a set of strings and probabilistic (or deterministic) rules to define how you travel from character
    to character. It is both a generating process (as how we often view Markov Chains) AND a verification process (as how we
    often view regular expressions and context free grammars).
    """
    name: str

### Combinations of many different types of languages ###
# class CrossProductLanguage(Language):
#     pass # TODO(Adriano) unclear what this exactly is

### CS-Inspired Languages, and mostly Markov Languages ###
class RegularLanguage(Language):
    pass

# class ContextFreeLanguage(Language):
#     pass # NOTE not implemented yet and not clear how to do the computations of state

# class AlgebraicGroup(Language):
#     pass # TODO(Adriano) not exactly clear what the question is here